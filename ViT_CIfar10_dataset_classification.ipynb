{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DrKalam/Demo/blob/main/ViT_CIfar10_dataset_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " import torch\n",
        " import torch.nn as nn\n",
        " import  torch.nn.functional as F\n",
        " import torch.optim as optim\n",
        " from torch.utils.data import DataLoader\n",
        " import torchvision\n",
        " from torchvision import datasets , transforms\n",
        " import numpy as np\n",
        " import random\n",
        " import matplotlib.pyplot as plt\n",
        "\n"
      ],
      "metadata": {
        "id": "vE19lnDrz25w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hjNd-18JFfo-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2 . set up device agnostic code\n"
      ],
      "metadata": {
        "id": "u5Eh1Jsjmoup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.__version__"
      ],
      "metadata": {
        "collapsed": true,
        "id": "PLvkzSYomDen",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "f8376554-af40-423e-c36a-e4a383ae8431"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.6.0+cu124'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torchvision.__version__"
      ],
      "metadata": {
        "collapsed": true,
        "id": "A4P_ZpCimOqm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "95fa6e8d-9ccc-463b-8ec8-7ddb23e9d130"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0.21.0+cu124'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "collapsed": true,
        "id": "hfhdmtUim_fH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfca4c8e-6c8a-44e0-8b61-3b2cd26a0f80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Jul 24 15:57:51 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   46C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "id": "FbVuagXMnORF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0041eea7-342f-4bad-a989-1ebde11a56f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 . Set the Seed"
      ],
      "metadata": {
        "id": "grTGWqOPnkTS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "random.seed(42)"
      ],
      "metadata": {
        "id": "RSbjN4-jnrtx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4 setting hyperparameters"
      ],
      "metadata": {
        "id": "8QxgJPl_sAn-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 128\n",
        "EPOCH = 10\n",
        "LEARNING_RATE = 3e-4\n",
        "PATCH_SIZE = 4\n",
        "NUM_CLASSES =10\n",
        "IMAGE_SIZE = 32\n",
        "CHANNELS = 3\n",
        "EMBED_DIM = 256\n",
        "NUM_HEAD = 8\n",
        "DEPTH = 6\n",
        "MLP_DIM = 512\n",
        "DROP_RATE = 0.1\n",
        "\n"
      ],
      "metadata": {
        "id": "PH06ykPum0-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. define image tranformations operation\n"
      ],
      "metadata": {
        "id": "DWjJA5yktYpj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5) ,(0.5))\n",
        "\n",
        "])"
      ],
      "metadata": {
        "id": "1R3ZbLUatj_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6 getting a dataset"
      ],
      "metadata": {
        "id": "kDS0Haf5u0iA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = datasets.CIFAR10(root = \"data\",\n",
        "                                 train = True,\n",
        "                                 download=True ,\n",
        "                                 transform= transform)"
      ],
      "metadata": {
        "id": "O4bh5Wppu72O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52147b7f-5cd8-4869-94de-43d28f77bb41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 42.8MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = datasets.CIFAR10(root = \"data\",\n",
        "                                 train = False,\n",
        "                                 download=True ,\n",
        "                                 transform= transform)"
      ],
      "metadata": {
        "id": "gx3tInF71ZbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset"
      ],
      "metadata": {
        "id": "gGUqosxN1mxI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18a6b65c-8cbb-427e-b754-859684896802"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset CIFAR10\n",
              "    Number of datapoints: 50000\n",
              "    Root location: data\n",
              "    Split: Train\n",
              "    StandardTransform\n",
              "Transform: Compose(\n",
              "               ToTensor()\n",
              "               Normalize(mean=0.5, std=0.5)\n",
              "           )"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset"
      ],
      "metadata": {
        "id": "P2E10NZc15-Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2dfc062-8e08-41d0-e7b2-dc2866152347"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset CIFAR10\n",
              "    Number of datapoints: 10000\n",
              "    Root location: data\n",
              "    Split: Test\n",
              "    StandardTransform\n",
              "Transform: Compose(\n",
              "               ToTensor()\n",
              "               Normalize(mean=0.5, std=0.5)\n",
              "           )"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7 converting datasets into dataloaders\n",
        "data into mini batches / batches(of 120 images)"
      ],
      "metadata": {
        "id": "a93kwFhP2FiX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "id": "iMmVq_lpn4ra",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "b69fdad0-ae38-4f38-d943-f35ed0b406b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(dataset= train_dataset,\n",
        "                          batch_size = BATCH_SIZE ,\n",
        "                          shuffle= True)\n",
        "test_loader = DataLoader(dataset= test_dataset,\n",
        "                          batch_size = BATCH_SIZE ,\n",
        "                          shuffle= False\n",
        "                         )\n"
      ],
      "metadata": {
        "id": "QHoo9z_I2Ra2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"DataLoader : {train_loader , test_loader}\")\n",
        "print(f\"Lenght of train_loader : {len(train_loader)} batches of { BATCH_SIZE}....\")\n",
        "print(f\"Lenght of test_loader : {len(test_loader)} batches of { BATCH_SIZE}....\")\n"
      ],
      "metadata": {
        "id": "mcAg3-KT6IrP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "204fbb6d-49b7-4024-a963-8fdbe59614a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataLoader : (<torch.utils.data.dataloader.DataLoader object at 0x7a5ef42c7550>, <torch.utils.data.dataloader.DataLoader object at 0x7a5ef43366d0>)\n",
            "Lenght of train_loader : 391 batches of 128....\n",
            "Lenght of test_loader : 79 batches of 128....\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#buiding VIsion transformer components"
      ],
      "metadata": {
        "id": "Q-5O4HMU6u_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "  def __init__(self,\n",
        "                img_size,\n",
        "                patch_size,\n",
        "                in_channels,\n",
        "                  embed_dim):\n",
        "    super().__init__()\n",
        "    self.patch_size = patch_size\n",
        "    self.proj = nn.Conv2d( in_channels= in_channels ,\n",
        "                          out_channels= embed_dim,\n",
        "                            kernel_size= patch_size,\n",
        "                            stride = patch_size,\n",
        "                            )\n",
        "    num_patches = (img_size // patch_size) **2\n",
        "    self.cls_token  = nn.Parameter(torch.randn(1,1, embed_dim))\n",
        "    self.pos_embed = nn.Parameter(torch.randn(1, 1 + num_patches, embed_dim))\n",
        "\n",
        "\n",
        "  def forward(self , x : torch.Tensor):\n",
        "    B = x.size(0)\n",
        "    x = self.proj(x) # ( B, E , H/p , W/P)\n",
        "    x = x.flatten(2).transpose(1,2) # (B ,N , E)\n",
        "    cls_token = self.cls_token.expand (B, -1 , -1)\n",
        "    x = torch.cat((cls_token, x), dim = 1)\n",
        "    x = x + self.pos_embed\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "UBeeHUnY64u5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#define MLP"
      ],
      "metadata": {
        "id": "XRMYMI6MXY6z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_features,\n",
        "                 hidden_features,\n",
        "                 drop_rate):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(in_features = in_features,\n",
        "                             out_features=hidden_features)\n",
        "\n",
        "\n",
        "\n",
        "        self.fc2 = nn.Linear(in_features = hidden_features,\n",
        "                             out_features = in_features)\n",
        "\n",
        "        self.dropout =  nn.Dropout(drop_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dropout(F.gelu(self.fc1(x)))\n",
        "        x = self.dropout(self.fc2(x))\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "0G0CF-FIXfnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#building tranformer encoder layer"
      ],
      "metadata": {
        "id": "ke_TTs52aG7W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncorderLayer(nn.Module):\n",
        "  def __init__ ( self, embed_dim, num_heads,mlp_dim,drop_rate):\n",
        "     super().__init__()\n",
        "     self.norm1 = nn.LayerNorm(embed_dim)\n",
        "     self.attn = nn.MultiheadAttention(embed_dim,num_heads,dropout = drop_rate , batch_first = True)\n",
        "     self.norm2 = nn.LayerNorm(embed_dim)\n",
        "     self.mlp = MLP(embed_dim,mlp_dim, drop_rate)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.attn(self.norm1(x) ,self.norm1(x), self.norm1(x)) [0]\n",
        "    x =  x + self.mlp(self.norm2(x))\n",
        "    return x"
      ],
      "metadata": {
        "id": "uLQhuLGgaGnw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#creating the class vision transformer\n",
        " ## by combining all the three above"
      ],
      "metadata": {
        "id": "aHIeHHJggpXE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "  def __init__(self,img_size,patch_size,in_channels, num_classes, embed_dim , depth,num_heads,mlp_dim , drop_rate):\n",
        "\n",
        "     super().__init__()\n",
        "     self.patch_embed = PatchEmbedding(img_size, patch_size,in_channels,embed_dim)\n",
        "     self.encoder = nn.Sequential(*[\n",
        "         TransformerEncorderLayer(embed_dim , num_heads, mlp_dim ,drop_rate)\n",
        "         for _ in range(depth)\n",
        "\n",
        "     ])\n",
        "     self.norm = nn.LayerNorm(embed_dim )\n",
        "     self.head = nn.Linear (embed_dim , num_classes) #act as classifier\n",
        "  def forward (self , x):\n",
        "    x = self.patch_embed(x)\n",
        "    x = self.encoder(x)\n",
        "    x = self.norm(x)\n",
        "    cls_token = x [:, 0]\n",
        "    return self.head(cls_token)"
      ],
      "metadata": {
        "id": "QjbXwOuyXYct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Instantiate model"
      ],
      "metadata": {
        "id": "n_F_nQLUmHe6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = VisionTransformer(\n",
        "                          IMAGE_SIZE , PATCH_SIZE , CHANNELS ,NUM_CLASSES, EMBED_DIM , DEPTH ,NUM_HEAD, MLP_DIM , DROP_RATE).to(device)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LPLYDLKemDBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "n0QSWCMy21St",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db154264-4aa0-41c6-d500-4466506fe16c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VisionTransformer(\n",
              "  (patch_embed): PatchEmbedding(\n",
              "    (proj): Conv2d(3, 256, kernel_size=(4, 4), stride=(4, 4))\n",
              "  )\n",
              "  (encoder): Sequential(\n",
              "    (0): TransformerEncorderLayer(\n",
              "      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "      (attn): MultiheadAttention(\n",
              "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
              "      )\n",
              "      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
              "        (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (1): TransformerEncorderLayer(\n",
              "      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "      (attn): MultiheadAttention(\n",
              "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
              "      )\n",
              "      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
              "        (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (2): TransformerEncorderLayer(\n",
              "      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "      (attn): MultiheadAttention(\n",
              "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
              "      )\n",
              "      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
              "        (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (3): TransformerEncorderLayer(\n",
              "      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "      (attn): MultiheadAttention(\n",
              "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
              "      )\n",
              "      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
              "        (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (4): TransformerEncorderLayer(\n",
              "      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "      (attn): MultiheadAttention(\n",
              "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
              "      )\n",
              "      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
              "        (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (5): TransformerEncorderLayer(\n",
              "      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "      (attn): MultiheadAttention(\n",
              "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
              "      )\n",
              "      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
              "        (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "  (head): Linear(in_features=256, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Defining a loss funtion and optimizer"
      ],
      "metadata": {
        "id": "nuMaSCkN5Fss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(params=model.parameters(),\n",
        "                             lr= LEARNING_RATE)"
      ],
      "metadata": {
        "id": "9vgLjfhD4YuV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer\n"
      ],
      "metadata": {
        "id": "vJJUGFK06O3k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb6bf838-034b-4bce-a4a6-99396ab993fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Adam (\n",
              "Parameter Group 0\n",
              "    amsgrad: False\n",
              "    betas: (0.9, 0.999)\n",
              "    capturable: False\n",
              "    differentiable: False\n",
              "    eps: 1e-08\n",
              "    foreach: None\n",
              "    fused: None\n",
              "    lr: 0.0003\n",
              "    maximize: False\n",
              "    weight_decay: 0\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion\n"
      ],
      "metadata": {
        "id": "8IPxplKo6Spe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae4f2b19-ff34-4bec-874a-47e54d46b29f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CrossEntropyLoss()"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10.defining a training loop funtion"
      ],
      "metadata": {
        "id": "hWuUS5Rt6wA2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, loader,optimizer, criterion) :\n",
        "  model.train()\n",
        "  total_loss , correct = 0,0\n",
        "  for x , y in loader :\n",
        "    #mving / sending the data into the target device\n",
        "    x ,y = x.to(device), y.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    # 1 . forward pass (model out put raw logits)\n",
        "    out = model(x)\n",
        "    #2 . calculate loss (per batch)\n",
        "    loss = criterion( out,y)\n",
        "    #3 perform backpropogation\n",
        "    loss.backward()\n",
        "    # 4. perform Gradient descent\n",
        "    optimizer.step()\n",
        "\n",
        "    total_loss += loss.item() * x.size(0)\n",
        "    correct += (out.argmax(1) == y).sum().item()\n",
        "  #loss is to be scladed\n",
        "  return total_loss / len(loader.dataset) , correct/ len(loader.dataset)\n",
        "\n"
      ],
      "metadata": {
        "id": "9E_ecY5g6V6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate (model , loader):\n",
        "  model.eval()\n",
        "  correct = 0\n",
        "  with torch.inference_mode():\n",
        "    for x , y in loader :\n",
        "      x ,y = x.to(device), y.to(device)\n",
        "      out = model(x)\n",
        "      correct += (out.argmax(dim=1) == y).sum().item()\n",
        "  return correct / len(loader.dataset)"
      ],
      "metadata": {
        "id": "zN1Mw47C6tj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### traning"
      ],
      "metadata": {
        "id": "T00nvaQOy6rK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm"
      ],
      "metadata": {
        "id": "4vK4wkeX0bnv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "train_accuracies, test_accuracies = [], []\n",
        "\n",
        "for epoch in tqdm(range(EPOCH)):\n",
        "    train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n",
        "    test_acc = evaluate(model, test_loader)\n",
        "\n",
        "    train_accuracies.append(train_acc)\n",
        "    test_accuracies.append(test_acc)\n",
        "\n",
        "    print(f\"Epoch: {epoch+1}/{EPOCH}, \"\n",
        "          f\"Train loss: {train_loss:.4f}, \"\n",
        "          f\"Train acc: {train_acc:.2f}%, \"\n",
        "          f\"Test acc: {test_acc:.2f}%\")\n"
      ],
      "metadata": {
        "id": "HnDdlh6oy-Xb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231,
          "referenced_widgets": [
            "16938ad37c764626aab122326b17625e",
            "deadacdda3e14a05ba23e89f78ff4c65",
            "fdd6ef938a634a10b0ed5684355e41cb",
            "8181567d6d194aaaaa431a8048ff7bf3",
            "88b8e967b4494deebb4bf6d6399a5a6e",
            "a657d92c38e04990acdcf1366d687bbf",
            "bc7e749e331f4ae7b5f83bd9f18e9e41",
            "2a980125bd534106a4dab962fdea459c",
            "6eb2853ffa194b8cabd5028cb209486b",
            "f046b716371b4cda852dd5d093bb8aa9",
            "93ac364e5f764b4599d876461dcb5d22"
          ]
        },
        "outputId": "d9fba0fc-b813-466b-e1a9-005f2a2fdd16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/10 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "16938ad37c764626aab122326b17625e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/10, Train loss: 1.7331, Train acc: 0.37%, Test acc: 0.48%\n",
            "Epoch: 2/10, Train loss: 1.3834, Train acc: 0.50%, Test acc: 0.53%\n",
            "Epoch: 3/10, Train loss: 1.2328, Train acc: 0.56%, Test acc: 0.57%\n",
            "Epoch: 4/10, Train loss: 1.1271, Train acc: 0.60%, Test acc: 0.58%\n",
            "Epoch: 5/10, Train loss: 1.0379, Train acc: 0.63%, Test acc: 0.61%\n",
            "Epoch: 6/10, Train loss: 0.9662, Train acc: 0.65%, Test acc: 0.62%\n",
            "Epoch: 7/10, Train loss: 0.8885, Train acc: 0.68%, Test acc: 0.62%\n",
            "Epoch: 8/10, Train loss: 0.8157, Train acc: 0.71%, Test acc: 0.61%\n",
            "Epoch: 9/10, Train loss: 0.7457, Train acc: 0.74%, Test acc: 0.63%\n",
            "Epoch: 10/10, Train loss: 0.6707, Train acc: 0.76%, Test acc: 0.63%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_accuracies"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLmUF0xIFiN_",
        "outputId": "0df4abb6-19c2-4d92-90b5-05ea09dee409"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.37082,\n",
              " 0.50392,\n",
              " 0.55842,\n",
              " 0.59672,\n",
              " 0.62786,\n",
              " 0.65416,\n",
              " 0.68238,\n",
              " 0.70788,\n",
              " 0.73518,\n",
              " 0.76212]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "test_accuracies\n"
      ],
      "metadata": {
        "id": "0lBtn8rnOJOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot accuracy\n",
        "plt.plot(train_accuracies, label=\"Train Accuracy\")\n",
        "plt.plot(test_accuracies, label=\"Test Accuracy\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.title(\"Training and Test Accuracy\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Cowz6TCON93d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random"
      ],
      "metadata": {
        "id": "eTXc9mzvHBFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_dataset)\n",
        "\n"
      ],
      "metadata": {
        "id": "FOnPOFmaOxjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset[0][0].unsqueeze(dim=0).shape"
      ],
      "metadata": {
        "id": "aCG_tgFsLXms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_and_plot_grid(model,\n",
        "                          dataset,\n",
        "                          classes,\n",
        "                          grid_size=3):\n",
        "    model.eval()\n",
        "    fig, axes = plt.subplots(grid_size, grid_size, figsize=(9, 9))\n",
        "    for i in range(grid_size):\n",
        "        for j in range(grid_size):\n",
        "            idx = random.randint(0, len(dataset) - 1)\n",
        "            img, true_label = dataset[idx]\n",
        "            input_tensor = img.unsqueeze(dim=0).to(device)\n",
        "            with torch.inference_mode():\n",
        "                output = model(input_tensor)\n",
        "                _, predicted = torch.max(output.data, 1)\n",
        "            img = img / 2 + 0.5\n",
        "            npimg = img.cpu().numpy()\n",
        "            axes[i, j].imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "            color = classes[true_label] == classes[predicted.item()]\n",
        "            if color:\n",
        "                c = \"g\"\n",
        "            else:\n",
        "                c = \"r\"\n",
        "            axes[i, j].set_title(f\"Truth: {classes[true_label]}\\n, Predicted: {classes[predicted.item()]}\", fontsize=10, c=c)\n",
        "            axes[i, j].axis(\"off\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Eb5XjlP-MYO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_and_plot_grid(model,\n",
        "                      test_dataset,\n",
        "                      train_dataset.classes, grid_size=3)"
      ],
      "metadata": {
        "id": "cAT_NgpjMw_3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "16938ad37c764626aab122326b17625e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_deadacdda3e14a05ba23e89f78ff4c65",
              "IPY_MODEL_fdd6ef938a634a10b0ed5684355e41cb",
              "IPY_MODEL_8181567d6d194aaaaa431a8048ff7bf3"
            ],
            "layout": "IPY_MODEL_88b8e967b4494deebb4bf6d6399a5a6e"
          }
        },
        "deadacdda3e14a05ba23e89f78ff4c65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a657d92c38e04990acdcf1366d687bbf",
            "placeholder": "​",
            "style": "IPY_MODEL_bc7e749e331f4ae7b5f83bd9f18e9e41",
            "value": "100%"
          }
        },
        "fdd6ef938a634a10b0ed5684355e41cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a980125bd534106a4dab962fdea459c",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6eb2853ffa194b8cabd5028cb209486b",
            "value": 10
          }
        },
        "8181567d6d194aaaaa431a8048ff7bf3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f046b716371b4cda852dd5d093bb8aa9",
            "placeholder": "​",
            "style": "IPY_MODEL_93ac364e5f764b4599d876461dcb5d22",
            "value": " 10/10 [08:10&lt;00:00, 49.90s/it]"
          }
        },
        "88b8e967b4494deebb4bf6d6399a5a6e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a657d92c38e04990acdcf1366d687bbf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc7e749e331f4ae7b5f83bd9f18e9e41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2a980125bd534106a4dab962fdea459c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6eb2853ffa194b8cabd5028cb209486b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f046b716371b4cda852dd5d093bb8aa9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93ac364e5f764b4599d876461dcb5d22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}